import asyncio
import random
import uuid
from datetime import datetime
from sqlalchemy.orm import Session
from typing import Optional
from pinecone_service import pinecone_service
from database import SessionLocal
from gemini_service import gemini_service
from conversation_service import conversation_service
from schemas import Message


class AutoDatasetGenerator:
    """Service tá»± Ä‘á»™ng táº¡o dataset training"""
    
    def __init__(self):
        self.is_running = False
        self.total_generated = 0
        self.current_session = None
        
    async def generate_single_conversation(
        self,
        db: Session
    ) -> dict:
        """
        FLOW:
        1. Táº¡o cÃ¢u chuyá»‡n tÃ³m táº¯t
        2. Convert story â†’ vector
        3. Check trÃ¹ng láº·p (similarity > 0.85)
        4. Náº¿u KHÃ”NG trÃ¹ng:
           - LÆ°u vector vÃ o Pinecone
           - ÄÆ°a story vÃ o prompt
           - Gemini táº¡o messages[]
           - LÆ°u messages vÃ o Neon
        
        Returns:
            dict: ThÃ´ng tin vá» conversation Ä‘Ã£ táº¡o
        """
        print(f"\n{'='*60}")
        print(f"ğŸ¯ Báº®T Äáº¦U táº¡o conversation má»›i")
        print(f"{'='*60}\n")
        
        try:
            # ===== BÆ¯á»šC 1: Táº¡o cÃ¢u chuyá»‡n tÃ³m táº¯t =====
            print(f"ğŸ“– BÆ¯á»šC 1: Táº¡o cÃ¢u chuyá»‡n tÃ³m táº¯t...")
            story = await gemini_service.generate_story_summary()
            
            # ===== BÆ¯á»šC 2: Convert story â†’ vector =====
            print(f"\nğŸ”¢ BÆ¯á»šC 2: Convert story â†’ vector...")
            embedding = await pinecone_service.get_story_embedding(story)
            
            if embedding is None:
                print(f"âš ï¸  KhÃ´ng táº¡o Ä‘Æ°á»£c embedding, bá» qua kiá»ƒm tra trÃ¹ng")
            
            # ===== BÆ¯á»šC 3: Check trÃ¹ng láº·p =====
            print(f"\nğŸ” BÆ¯á»šC 3: Kiá»ƒm tra trÃ¹ng láº·p...")
            is_duplicate, similarity = await pinecone_service.check_story_duplicate(
                story=story,
                embedding=embedding
            )
            
            if is_duplicate:
                print(f"âŒ CÃ¢u chuyá»‡n Bá»Š TRÃ™NG (similarity: {similarity:.4f} > 0.85)")
                print(f"â­ï¸  Bá» QUA cÃ¢u chuyá»‡n nÃ y\n")
                return {
                    "status": "skipped",
                    "reason": "duplicate",
                    "similarity": similarity,
                    "story": story
                }
            
            print(f"âœ… CÃ¢u chuyá»‡n Há»¢P Lá»† (similarity: {similarity:.4f} â‰¤ 0.85)")
            
            # ===== BÆ¯á»šC 4: Táº¡o messages[] tá»« story =====
            print(f"\nğŸ’¬ BÆ¯á»šC 4: Táº¡o há»™i thoáº¡i tá»« cÃ¢u chuyá»‡n...")
            messages = await gemini_service.generate_conversation_with_gemini(
                db=db,
                story_context=story
            )
            
            if not messages or not isinstance(messages, list):
                raise ValueError("Response tá»« Gemini khÃ´ng há»£p lá»‡!")
            
            print(f"âœ… ÄÃ£ nháº­n {len(messages)} messages tá»« Gemini")
            
            # Preview
            for i, msg in enumerate(messages[:4], 1):
                role = msg.get('role', 'unknown')
                content = msg.get('content', '')[:50]
                print(f"  [{i}] {role}: {content}...")
            if len(messages) > 4:
                print(f"  ... vÃ  {len(messages) - 4} messages khÃ¡c")
            
            # ===== BÆ¯á»šC 5: LÆ°u vÃ o Neon Database =====
            print(f"\nğŸ’¾ BÆ¯á»šC 5: LÆ°u conversation vÃ o Neon...")
            conversation = conversation_service.save_conversation(
                db=db,
                messages=messages
            )
            
            # ===== BÆ¯á»šC 6: LÆ°u vector vÃ o Pinecone =====
            print(f"\nğŸ”— BÆ¯á»šC 6: LÆ°u vector vÃ o Pinecone...")
            vector_id = await pinecone_service.save_story_vector(
                story=story,
                embedding=embedding,
                conversation_id=conversation.id
            )
            
            print(f"\nâœ… HOÃ€N THÃ€NH!")
            print(f"ğŸ“Š Conversation ID: {conversation.id}")
            print(f"ğŸ“ Tá»•ng messages: {len(messages)}")
            print(f"ğŸ”— Vector ID: {vector_id}")
            print(f"ğŸ“– Story: {story[:80]}...")
            print(f"{'='*60}\n")
            
            return {
                "id": conversation.id,
                "total_messages": len(messages),
                "messages": messages,
                "story": story,
                "vector_id": vector_id,
                "status": "success"
            }
            
        except Exception as e:
            print(f"\nâŒ Lá»–I khi táº¡o conversation")
            print(f"âŒ Chi tiáº¿t: {str(e)}\n")
            return {
                "status": "failed",
                "error": str(e)
            }

    async def run_continuous(
        self,
        total_conversations: Optional[int] = None,
        delay_between_conversations: int = 5
    ):
        self.is_running = True
        self.total_generated = 0
        
        try:
            count = 0
            while self.is_running:
                # Kiá»ƒm tra náº¿u Ä‘Ã£ Ä‘á»§ sá»‘ lÆ°á»£ng (náº¿u cÃ³ giá»›i háº¡n)
                if total_conversations and count >= total_conversations:
                    print(f"\nğŸ‰ HOÃ€N THÃ€NH! ÄÃ£ táº¡o {total_conversations} messages!")
                    break
                
                count += 1
                print(f"\n{'ğŸ”„'*40}")
                print(f"ğŸ“ MESSAGES #{count}/{total_conversations if total_conversations else 'âˆ'}")
                print(f"{'ğŸ”„'*40}")
                
                db = SessionLocal()
                try:
                    # Táº¡o 1 conversation báº±ng Gemini API
                    result = await self.generate_single_conversation(db=db)
                    
                    if result["status"] == "success":
                        self.total_generated += 1
                        print(f"âœ… Conversation #{count} Ä‘Ã£ lÆ°u thÃ nh cÃ´ng!")
                    elif result["status"] == "skipped":
                        print(f"â­ï¸  Conversation #{count} bá»‹ bá» qua: {result.get('reason', 'Unknown')}")
                        print(f"ğŸ“Š Tá»•ng messages Ä‘Ã£ táº¡o: {self.total_generated}")
                    else:
                        print(f"âŒ Messages #{count} tháº¥t báº¡i: {result.get('error', 'Unknown error')}")
                    
                except Exception as e:
                    print(f"âŒ Lá»—i khi táº¡o messages #{count}: {str(e)}")
                    
                finally:
                    db.close()
                
                # Delay trÆ°á»›c khi táº¡o messages tiáº¿p theo
                if self.is_running and (not total_conversations or count < total_conversations):
                    print(f"\nâ³ Chá» {delay_between_conversations}s trÆ°á»›c messages tiáº¿p theo...")
                    print(f"{'â”€'*80}\n")
                    await asyncio.sleep(delay_between_conversations)
                    
        except KeyboardInterrupt:
            print(f"\n\nâš ï¸  Nháº­n Ctrl+C - Äang dá»«ng an toÃ n...")
            self.is_running = False
            
        except Exception as e:
            print(f"\nâŒ Lá»–I NGHIÃŠM TRá»ŒNG: {str(e)}\n")
            self.is_running = False
        
        finally:
            self.is_running = False
            print("\n" + "="*80)
            print(f"ğŸ Káº¾T THÃšC CHÆ¯Æ NG TRÃŒNH")
            print(f"ğŸ“Š Tá»•ng messages Ä‘Ã£ táº¡o: {self.total_generated}")
            print(f"ğŸ’¾ Táº¥t cáº£ Ä‘Ã£ Ä‘Æ°á»£c lÆ°u vÃ o Neon Database")
            print("="*80 + "\n")
    
    def stop(self):
        """Dá»«ng quÃ¡ trÃ¬nh táº¡o dá»¯ liá»‡u"""
        self.is_running = False
        print("\nâ¸ï¸  ÄANG Dá»ªNG quÃ¡ trÃ¬nh táº¡o dá»¯ liá»‡u...\n")


# Singleton instance
auto_generator = AutoDatasetGenerator()